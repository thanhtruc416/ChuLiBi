# Function/Frame07_Cluster.py
from __future__ import annotations
from pathlib import Path
from typing import Iterable, Optional, List, Tuple

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import font_manager

from matplotlib.figure import Figure
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ================= PATH =================
# File n√†y ·ªü: Project/Function/Frame07_Cluster.py
project_root = Path(__file__).resolve().parents[1]
DATA_DIR = project_root / "Dataset" / "Output"
DATA_DIR = Path(os.getenv("DATA_DIR", str(DATA_DIR)))  # cho ph√©p override

NEEDED = ["df_scaled_model.csv", "df_raw_dashboard.csv"]
missing = [f for f in NEEDED if not (DATA_DIR / f).exists()]
if missing:
    raise FileNotFoundError(
        f"Kh√¥ng t√¨m th·∫•y {missing} trong {DATA_DIR}. "
        f"H√£y ki·ªÉm tra ƒë∆∞·ªùng d·∫´n ho·∫∑c ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng DATA_DIR."
    )

# ========== C·∫§U H√åNH ==========
LIKERT_GROUPS = {
    "pca_convenience": ["Ease and convenient_encoded", "Self Cooking_encoded", "Health Concern_encoded"],
    "pca_service_issue": ["Poor Hygiene_encoded", "Bad past experience_encoded", "Late Delivery_encoded"],
    "pca_deal_sensitive": ["More Offers and Discount_encoded", "Influence of rating_encoded"],
}

CLUSTER_FEATURES = [
    "Age", "Family size", "Restaurant Rating", "Delivery Rating",
    "No. of orders placed", "Delivery Time", "Order Value",
    "pca_convenience", "pca_service_issue", "pca_deal_sensitive",
]

# ========== HELPERS ==========
def _ensure_X_array(X):
    """B√≥c (X, use_cols) n·∫øu c√≥ v√† √©p v·ªÅ ndarray float"""
    if isinstance(X, (tuple, list)) and len(X) >= 1 and hasattr(X[0], "shape"):
        X = X[0]
    return np.asarray(X, dtype=float)

def load_scaled_dataset(csv_path: Path, id_col: str = "CustomerID") -> Tuple[pd.DataFrame, pd.Series]:
    df = pd.read_csv(csv_path)
    ids = df[id_col] if id_col in df.columns else pd.Series(range(len(df)))
    df = df.drop(columns=[id_col], errors="ignore")
    return df, ids

def attach_group_pca(df_scaled: pd.DataFrame, random_state: int = 42) -> pd.DataFrame:
    df = df_scaled.copy()
    for new_col, cols in LIKERT_GROUPS.items():
        miss = [c for c in cols if c not in df.columns]
        if miss:
            # thi·∫øu c·ªôt th√¨ b·ªè qua ƒë·ªÉ kh√¥ng v·ª° pipeline
            continue
        comp = PCA(n_components=1, random_state=random_state).fit_transform(df[cols])
        df[new_col] = comp
    return df

def select_X(df: pd.DataFrame, features: Optional[List[str]] = None) -> np.ndarray:
    """Tr·∫£ v·ªÅ ndarray 2D d√πng cho KMeans (UI kh√¥ng c·∫ßn s·ª≠a)."""
    cols_src = features if features is not None else CLUSTER_FEATURES
    use_cols = [c for c in cols_src if c in df.columns]
    if len(use_cols) < 2:
        raise RuntimeError(f"Qu√° √≠t bi·∫øn ƒë·ªÉ ph√¢n c·ª•m: {len(use_cols)}")
    return df[use_cols].to_numpy(dtype=float, copy=False)

# ========== KMEANS / EVAL ==========
def kmeans_labels(X, k: int, random_state: int = 42, n_init: int = 20) -> np.ndarray:
    X = _ensure_X_array(X)
    km = KMeans(n_clusters=k, random_state=random_state, n_init=n_init)
    return km.fit_predict(X)

def evaluate_k_range(X, k_min: int = 2, k_max: int = 11) -> Tuple[List[int], List[float], List[float]]:
    X = _ensure_X_array(X)
    Ks, inertias, sils = [], [], []
    for k in range(k_min, k_max):
        km = KMeans(n_clusters=k, random_state=42, n_init=20)
        lab = km.fit_predict(X)
        Ks.append(k)
        inertias.append(km.inertia_)
        sils.append(silhouette_score(X, lab))
    return Ks, inertias, sils

def counts_by_cluster(labels: Iterable[int], k: Optional[int] = None) -> List[int]:
    s = pd.Series(labels).value_counts().sort_index()
    if k is None:
        k = int(s.index.max()) + 1
    return [int(s.get(i, 0)) for i in range(k)]

# ========== FIGURES ==========
def figure_elbow_silhouette(X, k_min: int = 2, k_max: int = 11) -> Figure:
    X = _ensure_X_array(X)
    Ks, inertias, sils = evaluate_k_range(X, k_min, k_max)

    fig = Figure(figsize=(3.8, 2.0), dpi=120)
    # fig.set_constrained_layout(True)   # <-- xo√° d√≤ng n√†y
    ...
    fig.subplots_adjust(left=0.18, right=0.86, bottom=0.22, top=0.95)  # gi·ªØ d√≤ng n√†y
    ax = fig.add_subplot(111)
    ax2 = ax.twinx()

    l1, = ax.plot(Ks, inertias, "o-", linewidth=1.6, markersize=3.5, color="#644E94", label="Inertia (WCSS)")
    l2, = ax2.plot(Ks, sils,     "s-", linewidth=1.6, markersize=3.5, color="#BB95BB", label="Silhouette")

    for s in ax.spines.values():  s.set_color("#7A5FA5")
    for s in ax2.spines.values(): s.set_color("#7A5FA5")
    ax.grid(True, alpha=0.18, color="#7A5FA5", linewidth=0.6)

    ax.set_xticks(Ks)
    ax.tick_params(axis="both", labelsize=7, pad=1)
    ax2.tick_params(axis="y", labelsize=7, pad=1)
    ax.set_xlabel("k", fontsize=8, labelpad=2)
    ax.set_ylabel("Inertia", fontsize=8, color="#644E94")
    ax2.set_ylabel("Silhouette", fontsize=8, color="#BB95BB")

    leg = ax.legend([l1, l2], ["Inertia (WCSS)", "Silhouette"],
                    loc="upper right", bbox_to_anchor=(0.98, 0.98),
                    fontsize=7, frameon=True, fancybox=True, framealpha=0.9,
                    borderpad=0.3, labelspacing=0.25, handlelength=1.4)
    leg.get_frame().set_edgecolor("#7A5FA5")
    leg.get_frame().set_linewidth(0.7)
    leg.get_frame().set_facecolor("white")

    fig.subplots_adjust(left=0.18, right=0.86, bottom=0.22, top=0.95)
    return fig

def figure_cluster_distribution(labels: Iterable[int], scale: float = 1.2) -> Figure:
    s = pd.Series(labels).value_counts().sort_index()

    base_w, base_h = 3.35, 1.85
    fig = Figure(figsize=(base_w * scale, base_h * scale), dpi=120)
    ax = fig.add_subplot(111)
    fig.patch.set_alpha(0)

    colors = ["#FAE4F2", "#C6ABC5", "#644E94", "#ffcc99"][:len(s)]

    label_fs   = max(8, int(9 * scale))
    legend_fs  = max(8, int(8 * scale))
    edge_lw    = 0.6 * (0.9 + 0.25 * (scale - 1))

    wedges, _texts, autotexts = ax.pie(
        s.values,
        labels=None,
        autopct=lambda pct: "{:.1f}%\n({:.0f})".format(pct, pct/100.0*s.sum()),
        startangle=140,
        colors=colors,
        pctdistance=0.62,
        textprops={"fontsize": label_fs, "fontweight": "bold", "color": "black"},
        wedgeprops={"linewidth": edge_lw, "edgecolor": "white"},
        radius=1.0
    )
    ax.axis("equal")

    ax.legend(
        [f"Cluster {i+1}" for i in s.index],
        loc="upper center", bbox_to_anchor=(0.5, -0.05),
        ncol=len(s), fontsize=legend_fs, frameon=False
    )

    fig.subplots_adjust(left=0.02, right=0.98, top=0.98, bottom=0.15)
    return fig

def figure_pca_scatter(X, labels: Iterable[int]) -> Figure:
    X = _ensure_X_array(X)
    labels = np.asarray(list(labels))
    X2 = PCA(n_components=2, random_state=42).fit_transform(X)

    uniq = np.sort(np.unique(labels))
    idx_map = {lab: i for i, lab in enumerate(uniq)}
    disp = np.array([idx_map[l] + 1 for l in labels])
    hue_order = list(range(1, len(uniq) + 1))

    # üîπ tƒÉng chi·ªÅu ngang cho legend + gi·ªØ b·ªë c·ª•c c√¢n
    fig = Figure(figsize=(7.5, 3.0), dpi=120)
    fig.patch.set_alpha(0)
    ax = fig.add_subplot(111)

    palette = ["#FAE4F2", "#C6ABC5", "#644E94", "#ffcc99"][:len(uniq)]

    sns.scatterplot(
        x=X2[:, 0], y=X2[:, 1],
        hue=disp,
        hue_order=hue_order,
        palette=palette,
        s=26, edgecolor="#3a2a68", linewidth=0.35, ax=ax
    )

    # vi·ªÅn & l∆∞·ªõi
    for spine in ax.spines.values():
        spine.set_color("#7A5FA5")
    ax.grid(True, alpha=0.18, color="#7A5FA5", linewidth=0.7)

    # tr·ª•c
    ax.set_xlabel("PC1", fontsize=9, labelpad=6)
    ax.set_ylabel("PC2", fontsize=9, labelpad=12)

    # üîπ legend ra ngo√†i, nh∆∞ng √©p figure ch·ª´a ƒë·ªß kho·∫£ng tr·ªëng
    font_legend = font_manager.FontProperties(family="Crimson Pro", size=8)

    leg = ax.legend(
        title="Cluster",
        loc="center left",
        bbox_to_anchor=(1.06, 0.5),
        frameon=False,
        labelspacing=0.4,
        prop=font_legend,  # üëà d√πng prop thay v√¨ font
        title_fontsize=9
    )

    # üîπ ch·ª´a th√™m kho·∫£ng tr·∫Øng b√™n tr√°i & ph·∫£i ƒë·ªÉ kh√¥ng ƒë√® ch·ªØ
    fig.subplots_adjust(left=0.16, right=0.82, bottom=0.18, top=0.90)

    return fig




# ========== SAVE OUTPUTS ==========
def save_outputs(
    df_cluster: pd.DataFrame,
    df_raw: pd.DataFrame,
    use_cols: List[str],
    pca_cols: Optional[List[str]] = None,
) -> dict:
    pca_cols = pca_cols or []

    # profile mean tr√™n c√°c bi·∫øn d√πng ph√¢n c·ª•m
    cluster_profile = df_cluster.groupby("cluster")[use_cols].mean().round(3)
    cluster_profile["Count"] = df_cluster["cluster"].value_counts().sort_index().values

    # merge raw ƒë·ªÉ c√≥ s·ªë li·ªáu g·ªëc
    if "CustomerID" in df_raw.columns:
        merged = pd.merge(df_raw, df_cluster[["CustomerID", "cluster"]], on="CustomerID", how="left")
    else:
        merged = df_raw.copy()
        merged["cluster"] = df_cluster["cluster"].values

    # m√¥ t·∫£ PCA (th·∫•p/trung/cao)
    def label_pca(v: float, thresholds=(-0.3, 0.3)) -> str:
        return "Th·∫•p" if v <= thresholds[0] else ("Cao" if v >= thresholds[1] else "Trung b√¨nh")

    profile_real = merged.groupby("cluster")[list({*use_cols} - set(pca_cols))].mean().round(2)
    if pca_cols:
        pca_means = df_cluster.groupby("cluster")[pca_cols].mean().round(2)
        profile_real = pd.concat([profile_real, pca_means], axis=1)
    desc_profile = profile_real.copy()
    for c in pca_cols:
        desc_profile[c] = profile_real[c].apply(label_pca)
    desc_profile.rename(columns={
        "pca_convenience": "M·ª©c ƒë·ªô coi tr·ªçng s·ª± ti·ªán l·ª£i",
        "pca_service_issue": "V·∫•n ƒë·ªÅ d·ªãch v·ª•",
        "pca_deal_sensitive": "Nh·∫°y c·∫£m ∆∞u ƒë√£i/ƒë√°nh gi√°",
    }, inplace=True)

    # L∆∞u CHU·∫®N
    out_full    = DATA_DIR / "df_cluster_full.csv"
    out_profile = DATA_DIR / "cluster_profile_scaled.csv"
    out_desc    = DATA_DIR / "cluster_characteristics_descriptive.csv"

    df_cluster.to_csv(out_full, index=False)
    cluster_profile.to_csv(out_profile)
    desc_profile.to_csv(out_desc, index=False, encoding="utf-8-sig")

    return {
        "df_cluster_full": out_full,
        "cluster_profile_scaled": out_profile,
        "cluster_characteristics_descriptive": out_desc,
    }

# ========== RUN PIPELINE ==========
def run_pipeline(
    k_final: int = 3,
    feature_file: Optional[Path] = None,
) -> dict:
    """load ‚Üí PCA nh√≥m ‚Üí ch·ªçn features ‚Üí KMeans ‚Üí l∆∞u file ‚Üí figures."""
    df_scaled_model = pd.read_csv(DATA_DIR / "df_scaled_model.csv")
    df_raw          = pd.read_csv(DATA_DIR / "df_raw_dashboard.csv")

    ids = df_scaled_model["CustomerID"] if "CustomerID" in df_scaled_model.columns else pd.Series(range(len(df_scaled_model)))
    df_cluster = df_scaled_model.drop(columns=["CustomerID"], errors="ignore").copy()

    # PCA nh√≥m
    df_cluster = attach_group_pca(df_cluster, random_state=42)

    # ch·ªçn features (∆∞u ti√™n CSV c·ªôt 'feature' n·∫øu cung c·∫•p)
    features = None
    if feature_file is None:
        env_ff = os.getenv("FEATURE_FILE")
        if env_ff:
            feature_file = Path(env_ff)
    if feature_file and Path(feature_file).exists():
        feat = pd.read_csv(feature_file)
        if "feature" in feat.columns:
            features = feat["feature"].dropna().astype(str).tolist()
        else:
            features = feat.iloc[:, 0].dropna().astype(str).tolist()

    X = select_X(df_cluster, features)
    cols_src = features if features is not None else CLUSTER_FEATURES
    use_cols = [c for c in cols_src if c in df_cluster.columns]

    # g·ª£i √Ω K
    Ks, inertias, sils = evaluate_k_range(X, 2, 11)

    # fit cu·ªëi & g√°n nh√£n
    labels = kmeans_labels(X, k_final, random_state=42, n_init=20)
    df_cluster["cluster"] = labels
    df_cluster["CustomerID"] = ids

    # l∆∞u
    pca_cols = [c for c in ["pca_convenience", "pca_service_issue", "pca_deal_sensitive"] if c in df_cluster.columns]
    paths = save_outputs(df_cluster, df_raw, use_cols, pca_cols)

    # figures
    fig_elbow = figure_elbow_silhouette(X, 2, 11)
    fig_pie   = figure_cluster_distribution(labels, scale=1.2)
    fig_pca   = figure_pca_scatter(X, labels)

    return {
        "DATA_DIR": DATA_DIR,
        "use_cols": use_cols,
        "Ks": Ks,
        "inertias": inertias,
        "silhouettes": sils,
        "labels": labels,
        "fig_elbow": fig_elbow,
        "fig_pie": fig_pie,
        "fig_pca": fig_pca,
        "paths": paths,
    }

# ========== CLI ==========
if __name__ == "__main__":
    K = int(os.getenv("K_FINAL", "3"))
    FEATURE_FILE = os.getenv("FEATURE_FILE")
    info = run_pipeline(k_final=K, feature_file=Path(FEATURE_FILE) if FEATURE_FILE else None)

    print("DATA_DIR:", info["DATA_DIR"])
    print("Features d√πng:", info["use_cols"])
    print("Silhouette g·ª£i √Ω (k=2..10):", [round(v,3) for v in info["silhouettes"]])
    print("ƒê√£ l∆∞u:")
    for k, p in info["paths"].items():
        print(" -", k, "->", p)

    # V·∫Ω nhanh n·∫øu ch·∫°y standalone
    # AFTER

